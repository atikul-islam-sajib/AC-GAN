{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import joblib as pkl\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle(value = None, filename = None):\n",
    "    if value and filename:\n",
    "        pkl.dump(value = value, filename=filename)\n",
    "    else:\n",
    "        raise ValueError(\"value and filename are required\".capitalize())\n",
    "    \n",
    "def clean(path = None):\n",
    "    if path:\n",
    "        for file in os.listdir(path):\n",
    "            os.remove(os.path.join(path, file))\n",
    "    else:\n",
    "        raise ValueError(\"path is required\".capitalize())\n",
    "    \n",
    "def total_params(model = None):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_extract = \"../data/raw/\"\n",
    "to_save = \"../data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, image_path = None, batch_size = 64, image_size = 64, normalized = True):\n",
    "        self.image_path = image_path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.use_normalized = normalized\n",
    "\n",
    "    def unzip_images(self):\n",
    "        with zipfile.ZipFile(self.image_path, \"r\") as zip_ref:\n",
    "            if os.path.exists(to_extract):\n",
    "                zip_ref.extractall(to_extract)\n",
    "            else:\n",
    "                raise Exception(\"Extracting images failed\".capitalize())\n",
    "\n",
    "    def _normalized(self):\n",
    "        if self.use_normalized:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 5, 0.5]),\n",
    "                transforms.Grayscale(num_output_channels=1)\n",
    "            ])\n",
    "\n",
    "            return transform\n",
    "\n",
    "    @staticmethod\n",
    "    def class_to_idx(dataset = None):\n",
    "        if dataset is not None:\n",
    "            return dataset.class_to_idx\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        if os.path.exists(to_extract):\n",
    "            datasets = ImageFolder(root=os.path.join(to_extract, \"Dataset\"), transform=self._normalized())\n",
    "            dataloader = DataLoader(datasets, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            if os.path.exists(to_save):\n",
    "\n",
    "                try:\n",
    "                    pickle(value=dataloader, filename=os.path.join(to_save, \"dataloader.pkl\"))\n",
    "                    pickle(value=Loader.class_to_idx(dataset=datasets), filename=os.path.join(to_save, \"dataset.pkl\"))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            else:\n",
    "                raise Exception(\"Creating dataloader failed\".capitalize())\n",
    "        else:\n",
    "            raise Exception(\"Extracting images failed from the create dataloader method\".capitalize())\n",
    "\n",
    "        return dataloader, datasets.class_to_idx\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(\n",
    "        image_path=\"../Desktop/archive.zip\",\n",
    "        batch_size=64,\n",
    "        image_size=64,\n",
    "        normalized=True,\n",
    "    )\n",
    "    \n",
    "    loader.unzip_images()\n",
    "    dataloader, labels = loader.create_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 64, 64])\n",
      "3574856\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_space = 50, num_labels = 4, image_size = 64, in_channels = 1):\n",
    "        self.latent_space = latent_space\n",
    "        self.num_labels = num_labels\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.config_layers = [\n",
    "            (self.latent_space * 2, self.image_size * 8, 4, 1, 0, True, False),\n",
    "            (self.image_size * 8, self.image_size * 4, 4, 2, 1, True, False),\n",
    "            (self.image_size * 4, self.image_size * 2, 4, 2, 1, True, False),\n",
    "            (self.image_size * 2, self.image_size, 4, 2, 1, True, False),\n",
    "            (self.image_size, self.in_channels, 4, 2, 1, False, False),\n",
    "        ]\n",
    "        self.labels = nn.Embedding(num_embeddings=self.num_labels, embedding_dim=self.latent_space)\n",
    "        self.model = self.connected_layer(config_layers=self.config_layers)\n",
    "        \n",
    "    def connected_layer(self, config_layers = None):\n",
    "        if config_layers is not None:\n",
    "            layers = OrderedDict()\n",
    "            \n",
    "            for idx, (in_channels, out_channels, kernel_size, stride, padding, batch_norm, bias) in enumerate(config_layers[:-1]):\n",
    "                layers[f\"ConvTranspose{idx+1}\"] = nn.ConvTranspose2d(\n",
    "                    in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "                \n",
    "                if batch_norm:\n",
    "                    layers[f\"BatchNorm{idx+1}\"] = nn.BatchNorm2d(out_channels)\n",
    "                    \n",
    "                layers[f\"ReLU{idx+1}\"] = nn.ReLU(inplace=True)\n",
    "            \n",
    "            in_channels, out_channels, kernel_size, stride, padding, batch_norm, bias = config_layers[-1]\n",
    "            layers[f\"outConvTranspose\"] = nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "            layers[\"outLayer\"] = nn.Tanh()\n",
    "            \n",
    "            return nn.Sequential(layers)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"config layer should be defined\".capitalize())\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        labels = self.labels(labels)\n",
    "        labels = labels.view(labels.size(0), self.latent_space, 1, 1)\n",
    "        return self.model(torch.cat((noise, labels), dim=1))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net_G = Generator()\n",
    "    labels = torch.randint(0, 4, (64,))\n",
    "    noise = torch.randn(64, 50, 1, 1)\n",
    "    \n",
    "    print(net_G(noise, labels).shape)\n",
    "    print(total_params(model=net_G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1]) torch.Size([64, 4])\n",
      "2796288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/fpk79cm53rxgcj2gh5prtww00000gn/T/ipykernel_3006/286996701.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.sigmoid(real_or_fake.view(-1, 1)), F.log_softmax(labels.view(-1, self.num_labels))\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels = 1, num_labels = 4, image_size = 64):\n",
    "        self.in_channels = in_channels\n",
    "        self.num_labels = num_labels\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.config_layers = [\n",
    "            (self.in_channels, self.image_size, 4, 2, 1, False),\n",
    "            (self.image_size, self.image_size*2, 4, 2,1, True),\n",
    "            (self.image_size*2, self.image_size*4, 4, 2, 1, True),\n",
    "            (self.image_size*4, self.image_size*8, 4, 2, 1, True),\n",
    "            (self.image_size*8, 1 + self.num_labels, 4, 1, 0)\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        self.model = self.connected_layer(config_layers=self.config_layers)\n",
    "        \n",
    "    \n",
    "    def connected_layer(self, config_layers = None):\n",
    "        if config_layers is not None:\n",
    "            layers = OrderedDict()\n",
    "            \n",
    "            for idx, (in_channels, out_channels, kernel_size, stride, padding, batch_norm) in enumerate(config_layers[:-1]):\n",
    "                layers['conv{}'.format(idx+1)] = nn.Conv2d(\n",
    "                    in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "                if batch_norm:\n",
    "                    layers[f\"batchNorm{idx+1}\"] = nn.BatchNorm2d(num_features=out_channels)\n",
    "                    \n",
    "                layers[f\"leaky_relu{idx+1}\"] = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "                \n",
    "            (in_channels, out_channels, kernel_size, stride, padding) = config_layers[-1]\n",
    "            layers[\"out\"] = nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "            \n",
    "            return nn.Sequential(layers)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(\"config layer should be defined\".capitalize())\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        real_or_fake = output[:, 0:1]\n",
    "        labels = output[:, 1:]\n",
    "        return torch.sigmoid(real_or_fake.view(-1, 1)), F.log_softmax(labels.view(-1, self.num_labels))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    net_D = Discriminator()\n",
    "    \n",
    "    noise_data = torch.randn(64, 1, 64, 64)\n",
    "    real_fake, labels = net_D(noise_data)\n",
    "    print(real_fake.shape, labels.shape)\n",
    "    print(total_params(net_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "64, 1, 64, 64 -> real image\n",
    "64, 1, 64, 64 -> noise image that we will get from Generator\n",
    "\n",
    "target:\n",
    "64, 5\n",
    "0/1 -> 64, 1\n",
    "softmax -> 64, 4\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
