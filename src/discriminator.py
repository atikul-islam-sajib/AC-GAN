import sys
import logging
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    filemode="a",
    filename="./logs/discriminator.log",
)

sys.path.append("src/")

from utils import total_params


class Discriminator(nn.Module):
    """
    A discriminator module for distinguishing between real and generated images.

    This class implements a discriminator network for a GAN model, capable of distinguishing
    real images from those generated by a generator. It also categorizes images into one of
    several classes.

    **Methods**

    +------------------+------------------------------------------------------------------+
    | Method           | Description                                                      |
    +==================+==================================================================+
    | connected_layer  | Constructs the discriminator network layers based on             |
    |                  | configuration.                                                   |
    +------------------+------------------------------------------------------------------+
    | forward          | Defines the forward pass through the discriminator.              |
    +------------------+------------------------------------------------------------------+

    **Parameters**

    +--------------+--------+--------------------------------------------------------------+
    | Parameter    | Type   | Description                                                  |
    +==============+========+==============================================================+
    | in_channels  | int    | Number of input channels (e.g., 1 for grayscale, 3 for RGB). |
    +--------------+--------+--------------------------------------------------------------+
    | num_labels   | int    | Number of classes for classification.                        |
    +--------------+--------+--------------------------------------------------------------+
    | image_size   | int    | Base size of the convolutional filters.                      |
    +--------------+--------+--------------------------------------------------------------+
    """

    def __init__(self, in_channels=1, num_labels=4, image_size=64):
        self.in_channels = in_channels
        self.num_labels = num_labels
        self.image_size = image_size

        super(Discriminator, self).__init__()

        self.config_layers = [
            (self.in_channels, self.image_size, 4, 2, 1, False),
            (self.image_size, self.image_size * 2, 4, 2, 1, True),
            (self.image_size * 2, self.image_size * 4, 4, 2, 1, True),
            (self.image_size * 4, self.image_size * 8, 4, 2, 1, True),
            (self.image_size * 8, 1 + self.num_labels, 4, 1, 0),
        ]

        self.model = self.connected_layer(config_layers=self.config_layers)

    def connected_layer(self, config_layers=None):
        """
        Constructs the discriminator network layers from the provided configuration.

        **Parameters**

        +---------------+--------------------------+----------------------------------------+
        | Parameter     | Type                     | Description                            |
        +===============+==========================+========================================+
        | config_layers | list of tuples           | Configuration for each layer in the    |
        |               |                          | discriminator network.                 |
        +---------------+--------------------------+----------------------------------------+

        **Returns**

        A sequential model comprising all the configured layers.
        """
        if config_layers is not None:
            layers = OrderedDict()

            for idx, (
                in_channels,
                out_channels,
                kernel_size,
                stride,
                padding,
                batch_norm,
            ) in enumerate(config_layers[:-1]):
                layers["conv{}".format(idx + 1)] = nn.Conv2d(
                    in_channels=in_channels,
                    out_channels=out_channels,
                    kernel_size=kernel_size,
                    stride=stride,
                    padding=padding,
                    bias=False,
                )
                if batch_norm:
                    layers[f"batchNorm{idx+1}"] = nn.BatchNorm2d(
                        num_features=out_channels
                    )

                layers[f"leaky_relu{idx+1}"] = nn.LeakyReLU(
                    negative_slope=0.2, inplace=True
                )

            (in_channels, out_channels, kernel_size, stride, padding) = config_layers[
                -1
            ]
            layers["out"] = nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=kernel_size,
                stride=stride,
                padding=padding,
                bias=False,
            )

            return nn.Sequential(layers)

        else:
            raise ValueError("config layer should be defined".capitalize())

    def forward(self, x):
        """
        Defines the forward pass of the discriminator.

        **Parameters**

        +----------+------------+-----------------------------------------------+
        | Parameter| Type       | Description                                   |
        +==========+============+===============================================+
        | x        | torch.Tensor| The input image tensor.                       |
        +----------+------------+-----------------------------------------------+

        **Returns**

        A tuple containing the real/fake output and the class label logits.
        """
        output = self.model(x)
        real_or_fake = output[:, 0:1]
        labels = output[:, 1:]
        return torch.sigmoid(real_or_fake.view(-1, 1)), F.log_softmax(
            labels.view(-1, self.num_labels)
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Discriminator model defined".title())

    parser.add_argument(
        "--in_channels",
        type=int,
        default=1,
        help="Define the image rgb/gray".capitalize(),
    )
    parser.add_argument(
        "--image_size", type=int, default=64, help="Define the image size".capitalize()
    )
    parser.add_argument(
        "--num_labels",
        type=int,
        default=2,
        help="Define the number of labels".capitalize(),
    )
    parser.add_argument(
        "--netD", action="store_true", help="Define the netD".capitalize()
    )

    args = parser.parse_args()

    if args.netD:
        if args.in_channels and args.image_size and args.num_labels:
            logging.info("Discriminator model defined".capitalize())

            net_D = Discriminator(
                in_channels=args.in_channels,
                num_labels=args.num_labels,
                image_size=args.image_size,
            )
            logging.info("Discriminator model\n {}".format(net_D))
            logging.info("Total trainable parameters # {} ".format(total_params(net_D)))
        else:
            raise ValueError("All arguments are required".capitalize())

    else:
        raise ValueError("All arguments are required".capitalize())
